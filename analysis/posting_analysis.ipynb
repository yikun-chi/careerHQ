{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8fd6f9",
   "metadata": {},
   "source": [
    "# LinkedIn Job Posting Parsing for User Profile Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "676338db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import requests\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "170d69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API keyQ\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"Warning: OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "oai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facd466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (123849, 31)\n"
     ]
    }
   ],
   "source": [
    "df_posting = pd.read_csv('../analysis/data/LinkedIn_scrapping/postings.csv')\n",
    "print(f\"Shape: {df_posting.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a80ca845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected job_id: 3905882109\n",
      "Description preview: Dice is the leading career destination for tech experts at every stage of their careers. Our client, Request Technology, LLC, is seeking the following. Apply via Dice today!\n",
      "\n",
      "***We are unable to spons...\n",
      "\n",
      "Sending request to OpenAI Response API...\n",
      "\n",
      "✓ Success!\n",
      "\n",
      "Response ID: resp_0f0d411ffbe9cea10069718538f0b0819b8f23863570bd473e\n",
      "Model: gpt-5-mini-2025-08-07\n",
      "Status: completed\n",
      "\n",
      "--- Parsed Job Description ---\n",
      "Dice is the leading career destination for tech experts at every stage of their careers. Our client, Request Technology, LLC, is seeking the following. Apply via Dice today!\n",
      "\n",
      "***We are unable to sponsor as this is a permanent full-time role***\n",
      "\n",
      "A prestigious company is looking for a Oracle Applications DBA Tech Lead/Manager . This is a hands-on tech lead/manager. They will focus on all Oracle ERP applications and will focus heavily on Oracle EBS (11i/R12). This company is looking for someone with heavy Oracle Fusion Cloud experience.\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "You will support the following levels: Physical you will be responsible for the physical and technical oriented aspects e.g., storage, security, networking and more; Application you will handle all application-related issues (e.g., queries, users, embedded SQL s etc.)You will ensure database resources are sized properly and a design strategy is developed to make sure that the database is maintained at a healthy size.You will ensure availability and performance of multi database and application environments with very large volumes and sizes.Responsible to manage Oracle Cloud OCI platform and maintain the ongoing integrations and configurations to keep the Cloud platform in a healthy condition.You will manage or perform routine DBA tasks like database maintenance, backups, recovery, table space management, upgrades, etc.You will execute periodic health checks for databases and recommend changes that should be executed in the production environment to ensure optimal performance.You will interact and work with multiple infrastructure, Security and IT teams as part of environment setup, maintenance and support. This is an important function of this role.Plans and responds to service outages including backup and restore.Monitors system performance diagnosing software and hardware failures to resolutionProvides patch management and distribution capabilities to prevent new threats with minimized physical infrastructure impacts.Creates and tests data backups. Provides data cleansing services, verifies data integrity, and implements access controls.Performs database configuration, monitoring, and tuning.Routine maintenance including problem defect analysis and resolution, space, storage, object management, physical database layout, rollback segment management, and temporary space management.Supports logs and alerts, access rights and roles, database instance version control. New IDs, roles, authorizations, audits, locked/unlocked status, revocation.Performs memory usage monitoring, database parameter configuration, database resources, I/O distribution. Ensures system health, maintenance, technical security, recovery, and basic application technical tuning.Maintains Oracle Quarterly Releases and patch management schedules.Supports Systems Development, Migration, and Integration efforts as needed.Significant contributor to work plans for security analysis and recommendations\n",
      "\n",
      "--- Parsed Requirements ---\n",
      "Qualifications:\n",
      "\n",
      "Bachelor's Degree Computer Science or similar required10+ years Oracle EBS (11i/R12)Minimum five (5) years in ERP Cloud platform20+ years of experience as Oracle ERP APPS DBAMust have executed a minimum of two Upgrade cycles in the past. Will be responsible for Cutover Downtime reduction thru process improvements in each Upgrade iteration, Must have experience in EBS Application architecture, Concurrent managers, Workflow, layout of application on infrastructure, etc.Must have excellent knowledge in cloning, patching, backup recovery of ERP application including knowledge of ODI, FAW/ADW, Performance tuning, SQL profilingMust have hands on experience of RMAN, Backup Management and different Recovery techniques. Design and Implement disaster recovery plans.Support Oracle Cloud and EBS developers for custom coding, code migration, workflow and concurrent requestsKnowledge of system administration functionality within Oracle Cloud ERP, Unix Shell scripting skills on Oracle ODA s and OVM sAbility to manage and supervise DBA staff in the team.Responsible for providing day-to-day status and maintenance for Oracle EBS and ERP Cloud.Work with multiple application teams and developers.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a job from df_posting\n",
    "random_job = df_posting.sample(n=1).iloc[0]\n",
    "job_id = str(random_job['job_id'])\n",
    "description_text = random_job['description']\n",
    "\n",
    "print(f\"Selected job_id: {job_id}\")\n",
    "print(f\"Description preview: {description_text[:200]}...\")\n",
    "\n",
    "# Construct the Response API request\n",
    "request_body = {\n",
    "    \"model\": \"gpt-5-mini\",\n",
    "    \"instructions\": \"You are an expert at parsing job descriptions. Your task is to separate the job description into two parts: 1) The job description (what the role is about, responsibilities, and what the position entails), and 2) The requirements (qualifications, skills, experience, and criteria needed for the job). Do not add any new language, simply parse the existing words into those two categories.\",\n",
    "    \"input\": f\"Please parse the following job description and separate it into description and requirement:\\n\\n{description_text}\",\n",
    "    \"text\": {\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"job_parsing\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"description\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The core description of the job role, responsibilities, and what the position entails\"\n",
    "                    },\n",
    "                    \"requirement\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The requirements, qualifications, skills, and experience needed for the job\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"description\", \"requirement\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"max_output_tokens\": 16000\n",
    "}\n",
    "\n",
    "# Make API call\n",
    "url = \"https://api.openai.com/v1/responses\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(\"\\nSending request to OpenAI Response API...\")\n",
    "response = requests.post(url, headers=headers, json=request_body)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"\\n✓ Success!\")\n",
    "    print(f\"\\nResponse ID: {result.get('id')}\")\n",
    "    print(f\"Model: {result.get('model')}\")\n",
    "    print(f\"Status: {result.get('status')}\")\n",
    "    \n",
    "    # Check if response is incomplete\n",
    "    if result.get('status') == 'incomplete':\n",
    "        print(f\"⚠️ Warning: Response incomplete - {result.get('incomplete_details', {}).get('reason')}\")\n",
    "    \n",
    "    # Extract and display the parsed content\n",
    "    if 'output' in result and len(result['output']) > 0:\n",
    "        for output_item in result['output']:\n",
    "            if output_item.get('type') == 'message' and 'content' in output_item:\n",
    "                content_list = output_item['content']\n",
    "                for content_item in content_list:\n",
    "                    if content_item.get('type') == 'output_text' and 'text' in content_item:\n",
    "                        output_text = content_item['text']\n",
    "                        parsed_data = json.loads(output_text)\n",
    "                        print(\"\\n--- Parsed Job Description ---\")\n",
    "                        print(parsed_data.get('description', ''))\n",
    "                        print(\"\\n--- Parsed Requirements ---\")\n",
    "                        print(parsed_data.get('requirement', ''))\n",
    "                        break\n",
    "                break\n",
    "        else:\n",
    "            print(\"\\n⚠️ No text content found in output\")\n",
    "            print(\"Output items:\", [item.get('type') for item in result['output']])\n",
    "    else:\n",
    "        print(\"\\n⚠️ No output found in response\")\n",
    "else:\n",
    "    print(f\"\\n✗ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40918b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total requests created: 123842\n",
      "Created openai_batch_requests_batch1.jsonl: 30000 requests\n",
      "Created openai_batch_requests_batch2.jsonl: 30000 requests\n",
      "Created openai_batch_requests_batch3.jsonl: 30000 requests\n",
      "Created openai_batch_requests_batch4.jsonl: 30000 requests\n",
      "Created openai_batch_requests_batch5.jsonl: 3842 requests\n",
      "\n",
      "Total batch files: 5\n"
     ]
    }
   ],
   "source": [
    "# Create batch requests\n",
    "batch_requests = []\n",
    "\n",
    "for idx, row in df_posting.iterrows():\n",
    "    job_id = str(row['job_id'])\n",
    "    description_text = row['description']\n",
    "    \n",
    "    # Skip if description is empty or null\n",
    "    if pd.isna(description_text) or not description_text:\n",
    "        continue\n",
    "    \n",
    "    # Create the request using Response API format\n",
    "    request = {\n",
    "        \"custom_id\": job_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/responses\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-5-mini\",\n",
    "            \"instructions\": \"You are an expert at parsing job descriptions. Your task is to separate the job description into two parts: 1) The job description (what the role is about, responsibilities, and what the position entails), and 2) The requirements (qualifications, skills, experience, and criteria needed for the job). Do not add any new language, simply parse the existing words into those two categories.\",\n",
    "            \"input\": f\"Please parse the following job description and separate it into description and requirement:\\n\\n{description_text}\",\n",
    "            \"text\": {\n",
    "                \"format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"name\": \"job_parsing\",\n",
    "                    \"strict\": True,\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The core description of the job role, responsibilities, and what the position entails\"\n",
    "                            },\n",
    "                            \"requirement\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The requirements, qualifications, skills, and experience needed for the job\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"description\", \"requirement\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"max_output_tokens\": 16000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    batch_requests.append(request)\n",
    "\n",
    "print(f\"Total requests created: {len(batch_requests)}\")\n",
    "\n",
    "# Split into batches of max 30,000 each\n",
    "batch_size = 30000\n",
    "num_batches = (len(batch_requests) + batch_size - 1) // batch_size\n",
    "\n",
    "batch_files = []\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(batch_requests))\n",
    "    \n",
    "    output_file = f'openai_batch_requests_batch{batch_num + 1}.jsonl'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for request in batch_requests[start_idx:end_idx]:\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    batch_files.append(output_file)\n",
    "    print(f\"Created {output_file}: {end_idx - start_idx} requests\")\n",
    "\n",
    "print(f\"\\nTotal batch files: {len(batch_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4ce7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Batch Jobs Summary - 1 batches submitted\n",
      "================================================================================\n",
      "File                                          File ID                             Batch ID\n",
      "--------------------------------------------------------------------------------\n",
      "openai_batch_requests_batch1.jsonl            file-4XNkmcJVft1RSFzjiGVAbU         batch_69718a39dd888190bca7db2fe207a090\n"
     ]
    }
   ],
   "source": [
    "# Submit batch files to OpenAI Batch API\n",
    "\n",
    "batch_jobs = []\n",
    "\n",
    "# only submit one for prototyping \n",
    "for batch_file in batch_files[0:1]:\n",
    "    # Upload the batch input file\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        batch_input_file = oai_client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    \n",
    "    # Create the batch job\n",
    "    batch_job = oai_client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/responses\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"Job description parsing for {batch_file}\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    batch_jobs.append({\n",
    "        'file': batch_file,\n",
    "        'file_id': batch_input_file.id,\n",
    "        'batch_id': batch_job.id,\n",
    "        'status': batch_job.status\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Batch Jobs Summary - {len(batch_jobs)} batches submitted\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'File':<45} {'File ID':<35} {'Batch ID'}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for job in batch_jobs:\n",
    "    print(f\"{job['file']:<45} {job['file_id']:<35} {job['batch_id']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
